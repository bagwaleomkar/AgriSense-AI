"""
AgriSense AI - Feature Engineering for Crop Price Prediction
This script creates advanced features for ML model training
"""

import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("FEATURE ENGINEERING FOR CROP PRICE PREDICTION")
print("=" * 80)

# ============================================================================
# STEP 1: LOAD MERGED DATASET
# ============================================================================
print("\n" + "=" * 80)
print("STEP 1: LOADING MERGED DATASET")
print("=" * 80)

df = pd.read_csv('Merged_Crop_Weather_Data.csv')
print(f"âœ“ Dataset loaded: {df.shape}")

# Convert date columns
df['Date_crop'] = pd.to_datetime(df['Date_crop'])
df['Date_weather'] = pd.to_datetime(df['Date_weather'])

# Sort by district, commodity, and date for proper time series features
df = df.sort_values(['District', 'Commodity', 'Date_crop']).reset_index(drop=True)
print(f"âœ“ Data sorted by District, Commodity, and Date")

print(f"\nOriginal features: {df.shape[1]}")

# ============================================================================
# STEP 2: ROLLING AVERAGE FEATURES (7-DAY AND 30-DAY)
# ============================================================================
print("\n" + "=" * 80)
print("STEP 2: CREATING ROLLING AVERAGE FEATURES")
print("=" * 80)

print("""
ðŸ“Š WHY ROLLING AVERAGES ARE IMPORTANT:

1. SMOOTHING SHORT-TERM FLUCTUATIONS:
   â€¢ Daily prices can be volatile due to temporary market conditions
   â€¢ Rolling averages smooth out noise and reveal underlying trends
   â€¢ Help models focus on genuine patterns rather than random spikes

2. CAPTURING MOMENTUM:
   â€¢ 7-day average captures recent price momentum (short-term trend)
   â€¢ 30-day average captures medium-term trend direction
   â€¢ Relationship between short and long averages indicates trend strength

3. TREND IDENTIFICATION:
   â€¢ When 7-day avg > 30-day avg â†’ Upward trend (bullish signal)
   â€¢ When 7-day avg < 30-day avg â†’ Downward trend (bearish signal)
   â€¢ Crossovers indicate potential trend reversals

4. REDUCING OVERFITTING:
   â€¢ Smoothed values are more stable and generalizable
   â€¢ Help model learn from patterns rather than noise
""")

# Calculate rolling averages for each district-commodity combination
df['Price_MA_7'] = df.groupby(['District', 'Commodity'])['Modal_Price'].transform(
    lambda x: x.rolling(window=7, min_periods=1).mean()
)

df['Price_MA_30'] = df.groupby(['District', 'Commodity'])['Modal_Price'].transform(
    lambda x: x.rolling(window=30, min_periods=1).mean()
)

# Calculate moving average convergence/divergence indicator
df['Price_MA_Ratio'] = df['Price_MA_7'] / df['Price_MA_30']

print(f"âœ“ Created 7-day rolling average: Price_MA_7")
print(f"âœ“ Created 30-day rolling average: Price_MA_30")
print(f"âœ“ Created MA ratio indicator: Price_MA_Ratio")

print(f"\nSample values:")
print(f"  Original Price: {df['Modal_Price'].iloc[100]:.2f}")
print(f"  7-day MA: {df['Price_MA_7'].iloc[100]:.2f}")
print(f"  30-day MA: {df['Price_MA_30'].iloc[100]:.2f}")
print(f"  MA Ratio: {df['Price_MA_Ratio'].iloc[100]:.3f}")

# ============================================================================
# STEP 3: PRICE CHANGE PERCENTAGE FEATURES
# ============================================================================
print("\n" + "=" * 80)
print("STEP 3: CREATING PRICE CHANGE PERCENTAGE FEATURES")
print("=" * 80)

print("""
ðŸ“Š WHY PRICE CHANGE PERCENTAGE IS IMPORTANT:

1. RELATIVE MAGNITUDE OF CHANGE:
   â€¢ Absolute price changes can be misleading (â‚¹100 change means different things
     for â‚¹1000 vs â‚¹5000 crops)
   â€¢ Percentage change normalizes across different price levels
   â€¢ Allows comparison across different commodities

2. VOLATILITY MEASUREMENT:
   â€¢ Large percentage changes indicate high volatility/risk
   â€¢ Helps identify unstable market conditions
   â€¢ Important for risk assessment and forecasting uncertainty

3. MOMENTUM INDICATOR:
   â€¢ Consistent positive changes indicate strong upward momentum
   â€¢ Negative changes signal downward pressure
   â€¢ Rate of change helps predict acceleration/deceleration

4. FEATURE SCALING:
   â€¢ Percentages naturally bounded and comparable
   â€¢ Easier for models to learn from normalized values
   â€¢ Reduces bias from absolute price levels
""")

# Calculate daily price change percentage
df['Price_Change_Pct'] = df.groupby(['District', 'Commodity'])['Modal_Price'].pct_change() * 100

# Calculate 7-day price change percentage
df['Price_Change_7d_Pct'] = df.groupby(['District', 'Commodity'])['Modal_Price'].transform(
    lambda x: ((x - x.shift(7)) / x.shift(7)) * 100
)

# Calculate 30-day price change percentage
df['Price_Change_30d_Pct'] = df.groupby(['District', 'Commodity'])['Modal_Price'].transform(
    lambda x: ((x - x.shift(30)) / x.shift(30)) * 100
)

# Calculate volatility (rolling standard deviation of price changes)
df['Price_Volatility'] = df.groupby(['District', 'Commodity'])['Price_Change_Pct'].transform(
    lambda x: x.rolling(window=7, min_periods=1).std()
)

print(f"âœ“ Created daily price change: Price_Change_Pct")
print(f"âœ“ Created 7-day price change: Price_Change_7d_Pct")
print(f"âœ“ Created 30-day price change: Price_Change_30d_Pct")
print(f"âœ“ Created price volatility: Price_Volatility")

# ============================================================================
# STEP 4: MONTHLY AND SEASONAL FEATURES
# ============================================================================
print("\n" + "=" * 80)
print("STEP 4: CREATING MONTHLY AND SEASONAL FEATURES")
print("=" * 80)

print("""
ðŸ“Š WHY TEMPORAL FEATURES ARE IMPORTANT:

1. SEASONALITY CAPTURE:
   â€¢ Agriculture is highly seasonal (planting, growing, harvest cycles)
   â€¢ Different crops have different harvest seasons
   â€¢ Weather patterns follow seasonal cycles

2. CYCLICAL PATTERNS:
   â€¢ Sin/Cos encoding preserves cyclical nature (Dec and Jan are close)
   â€¢ Linear encoding would treat Jan(1) and Dec(12) as far apart
   â€¢ Helps model understand periodic patterns

3. MARKET DYNAMICS:
   â€¢ Festival seasons affect demand (Diwali, harvest festivals)
   â€¢ End of season often sees price drops (oversupply)
   â€¢ Quarter-level trends capture longer economic cycles

4. FEATURE RICHNESS:
   â€¢ Multiple representations give model flexibility
   â€¢ Can learn different patterns at different time scales
   â€¢ Improves model's ability to generalize
""")

# Extract temporal features
df['Day_of_Week'] = df['Date_crop'].dt.dayofweek
df['Day_of_Month'] = df['Date_crop'].dt.day
df['Week_of_Year'] = df['Date_crop'].dt.isocalendar().week
df['Quarter'] = df['Date_crop'].dt.quarter

# Cyclical encoding for month (sin and cos to preserve cyclical nature)
df['Month_Sin'] = np.sin(2 * np.pi * df['Month'] / 12)
df['Month_Cos'] = np.cos(2 * np.pi * df['Month'] / 12)

# Day of year (1-365)
df['Day_of_Year'] = df['Date_crop'].dt.dayofyear

# Season encoding (Maharashtra seasons)
def get_season(month):
    if month in [12, 1, 2]:
        return 'Winter'
    elif month in [3, 4, 5]:
        return 'Summer'
    elif month in [6, 7, 8, 9]:
        return 'Monsoon'
    else:  # 10, 11
        return 'Post_Monsoon'

df['Season'] = df['Month'].apply(get_season)

# One-hot encode season
season_dummies = pd.get_dummies(df['Season'], prefix='Season')
df = pd.concat([df, season_dummies], axis=1)

# Harvest season indicator (varies by crop, using general pattern)
# Rabi crops: Oct-March (harvest Mar-Apr)
# Kharif crops: Jun-Sep (harvest Oct-Nov)
def is_harvest_season(month, commodity):
    # Simplified logic - can be refined per commodity
    if commodity.lower() in ['wheat', 'gram', 'mustard']:  # Rabi crops
        return 1 if month in [3, 4] else 0
    elif commodity.lower() in ['rice', 'bajra', 'jowar', 'maize']:  # Kharif crops
        return 1 if month in [10, 11] else 0
    else:
        return 0

df['Is_Harvest_Season'] = df.apply(lambda x: is_harvest_season(x['Month'], x['Commodity']), axis=1)

print(f"âœ“ Created day of week: Day_of_Week (0=Monday, 6=Sunday)")
print(f"âœ“ Created day of month: Day_of_Month")
print(f"âœ“ Created week of year: Week_of_Year")
print(f"âœ“ Created quarter: Quarter")
print(f"âœ“ Created cyclical month encoding: Month_Sin, Month_Cos")
print(f"âœ“ Created day of year: Day_of_Year")
print(f"âœ“ Created season categories with one-hot encoding")
print(f"âœ“ Created harvest season indicator: Is_Harvest_Season")

# ============================================================================
# STEP 5: RAINFALL DEVIATION FROM MONTHLY AVERAGE
# ============================================================================
print("\n" + "=" * 80)
print("STEP 5: CREATING RAINFALL DEVIATION FEATURES")
print("=" * 80)

print("""
ðŸ“Š WHY RAINFALL DEVIATION IS IMPORTANT:

1. ANOMALY DETECTION:
   â€¢ Absolute rainfall doesn't tell the full story
   â€¢ What matters is deviation from normal/expected rainfall
   â€¢ Excess or deficit rainfall both impact crop yields and prices

2. CONTEXT-AWARE WEATHER IMPACT:
   â€¢ 50mm rain in monsoon = normal (expected)
   â€¢ 50mm rain in winter = exceptional (unexpected impact)
   â€¢ Deviation captures the surprise factor

3. REGIONAL NORMALIZATION:
   â€¢ Different districts have different rainfall patterns
   â€¢ Deviation normalizes across regions
   â€¢ 10mm in drought-prone area â‰  10mm in high-rainfall area

4. CROP STRESS INDICATOR:
   â€¢ Large positive deviation â†’ flooding risk, disease
   â€¢ Large negative deviation â†’ drought stress, yield reduction
   â€¢ Both extremes typically increase prices (reduced supply or panic)

5. PREDICTIVE POWER:
   â€¢ Historical deviations help model learn threshold effects
   â€¢ Non-linear relationships with prices easier to capture
   â€¢ Better than absolute rainfall for price prediction
""")

# Calculate monthly average rainfall by district
monthly_avg_rainfall = df.groupby(['District', 'Month'])['Daily_Rainfall_mm'].transform('mean')
df['Rainfall_Monthly_Avg'] = monthly_avg_rainfall

# Calculate deviation from monthly average
df['Rainfall_Deviation'] = df['Daily_Rainfall_mm'] - df['Rainfall_Monthly_Avg']

# Calculate percentage deviation
df['Rainfall_Deviation_Pct'] = (df['Rainfall_Deviation'] / (df['Rainfall_Monthly_Avg'] + 0.001)) * 100

# Calculate cumulative rainfall for the month
df['Year_Month'] = df['Date_crop'].dt.to_period('M')
df['Cumulative_Rainfall_Month'] = df.groupby(['District', 'Year_Month'])['Daily_Rainfall_mm'].cumsum()

# Rainfall categories based on deviation
def rainfall_category(deviation_pct):
    if deviation_pct < -50:
        return 'Severe_Deficit'
    elif deviation_pct < -20:
        return 'Deficit'
    elif deviation_pct < 20:
        return 'Normal'
    elif deviation_pct < 50:
        return 'Excess'
    else:
        return 'Severe_Excess'

df['Rainfall_Category'] = df['Rainfall_Deviation_Pct'].apply(rainfall_category)

# One-hot encode rainfall category
rainfall_dummies = pd.get_dummies(df['Rainfall_Category'], prefix='Rainfall')
df = pd.concat([df, rainfall_dummies], axis=1)

print(f"âœ“ Created monthly average rainfall: Rainfall_Monthly_Avg")
print(f"âœ“ Created rainfall deviation: Rainfall_Deviation")
print(f"âœ“ Created percentage deviation: Rainfall_Deviation_Pct")
print(f"âœ“ Created cumulative monthly rainfall: Cumulative_Rainfall_Month")
print(f"âœ“ Created rainfall categories with one-hot encoding")

print(f"\nSample values:")
sample_idx = 500
print(f"  Actual Rainfall: {df['Daily_Rainfall_mm'].iloc[sample_idx]:.2f} mm")
print(f"  Monthly Average: {df['Rainfall_Monthly_Avg'].iloc[sample_idx]:.2f} mm")
print(f"  Deviation: {df['Rainfall_Deviation'].iloc[sample_idx]:.2f} mm")
print(f"  Deviation %: {df['Rainfall_Deviation_Pct'].iloc[sample_idx]:.2f}%")

# ============================================================================
# STEP 6: LAG FEATURES FOR PRICES
# ============================================================================
print("\n" + "=" * 80)
print("STEP 6: CREATING LAG FEATURES FOR PRICES")
print("=" * 80)

print("""
ðŸ“Š WHY LAG FEATURES ARE IMPORTANT:

1. TIME SERIES AUTOCORRELATION:
   â€¢ Prices exhibit autocorrelation (today's price related to yesterday's)
   â€¢ Past prices contain information about future prices
   â€¢ Lag features explicitly provide historical context

2. MOMENTUM AND INERTIA:
   â€¢ Markets don't change instantly - there's inertia
   â€¢ Recent price history indicates current momentum
   â€¢ Multiple lags capture different time horizons

3. LEADING INDICATORS:
   â€¢ 1-day lag captures immediate past (strong correlation)
   â€¢ 7-day lag captures weekly patterns
   â€¢ 30-day lag captures monthly trends
   â€¢ Multiple lags help model learn temporal dependencies

4. FEATURE INTERACTIONS:
   â€¢ Combination of lags creates complex patterns
   â€¢ Model can learn from differences between lags
   â€¢ Captures acceleration/deceleration of price changes

5. AVOIDING DATA LEAKAGE:
   â€¢ Lags ensure we only use past information
   â€¢ Critical for time series to avoid future data contamination
   â€¢ Maintains temporal integrity of predictions

6. STATISTICAL FOUNDATION:
   â€¢ Autoregressive models (AR, ARIMA) rely on lags
   â€¢ Proven effective in time series forecasting
   â€¢ Provides baseline features for any time series model
""")

# Create lag features for modal price (1, 3, 7, 14, 30 days)
lag_periods = [1, 3, 7, 14, 30]

for lag in lag_periods:
    df[f'Price_Lag_{lag}d'] = df.groupby(['District', 'Commodity'])['Modal_Price'].shift(lag)
    print(f"âœ“ Created lag feature: Price_Lag_{lag}d")

# Create lag features for arrivals
for lag in [1, 7, 14]:
    df[f'Arrivals_Lag_{lag}d'] = df.groupby(['District', 'Commodity'])['Arrivals'].shift(lag)
    print(f"âœ“ Created arrivals lag feature: Arrivals_Lag_{lag}d")

# Create lag features for weather variables (7-day lag)
weather_vars = ['Daily_Rainfall_mm', 'Max_Temp_C', 'Min_Temp_C', 'Avg_Humidity_%']
for var in weather_vars:
    df[f'{var}_Lag_7d'] = df.groupby(['District'])[var].shift(7)

# Calculate differences between lags (price change indicators)
df['Price_Diff_1_7'] = df['Price_Lag_1d'] - df['Price_Lag_7d']
df['Price_Diff_7_30'] = df['Price_Lag_7d'] - df['Price_Lag_30d']

print(f"âœ“ Created lag differences: Price_Diff_1_7, Price_Diff_7_30")

# ============================================================================
# STEP 7: ADDITIONAL ADVANCED FEATURES
# ============================================================================
print("\n" + "=" * 80)
print("STEP 7: CREATING ADDITIONAL ADVANCED FEATURES")
print("=" * 80)

# Price spread (difference between max and min price)
df['Price_Spread'] = df['Max_Price'] - df['Min_Price']
df['Price_Spread_Pct'] = (df['Price_Spread'] / df['Modal_Price']) * 100

# Temperature range
df['Temp_Range'] = df['Max_Temp_C'] - df['Min_Temp_C']

# Weather comfort index (simplified)
df['Weather_Comfort_Index'] = (df['Max_Temp_C'] + df['Min_Temp_C']) / 2 - df['Avg_Humidity_%'] / 10

# Price position (where modal price sits between min and max)
df['Price_Position'] = (df['Modal_Price'] - df['Min_Price']) / (df['Price_Spread'] + 0.001)

# Log transformations for skewed variables
df['Log_Modal_Price'] = np.log1p(df['Modal_Price'])
df['Log_Arrivals'] = np.log1p(df['Arrivals'])

# Interaction features
df['Rainfall_Temp_Interaction'] = df['Daily_Rainfall_mm'] * df['Max_Temp_C']
df['Humidity_Temp_Interaction'] = df['Avg_Humidity_%'] * df['Max_Temp_C']

print(f"âœ“ Created price spread features")
print(f"âœ“ Created temperature range")
print(f"âœ“ Created weather comfort index")
print(f"âœ“ Created price position indicator")
print(f"âœ“ Created log-transformed features")
print(f"âœ“ Created interaction features")

# ============================================================================
# STEP 8: ENCODE CATEGORICAL VARIABLES
# ============================================================================
print("\n" + "=" * 80)
print("STEP 8: ENCODING CATEGORICAL VARIABLES")
print("=" * 80)

# District encoding (label encoding)
from sklearn.preprocessing import LabelEncoder

le_district = LabelEncoder()
df['District_Encoded'] = le_district.fit_transform(df['District'])

# Commodity encoding
le_commodity = LabelEncoder()
df['Commodity_Encoded'] = le_commodity.fit_transform(df['Commodity'])

# Market encoding
le_market = LabelEncoder()
df['Market_Encoded'] = le_market.fit_transform(df['Market'])

print(f"âœ“ Encoded District: {df['District'].nunique()} unique values")
print(f"âœ“ Encoded Commodity: {df['Commodity'].nunique()} unique values")
print(f"âœ“ Encoded Market: {df['Market'].nunique()} unique values")

# ============================================================================
# STEP 9: HANDLE MISSING VALUES AND SAVE
# ============================================================================
print("\n" + "=" * 80)
print("STEP 9: HANDLING MISSING VALUES AND SAVING")
print("=" * 80)

print(f"\nMissing values before handling:")
missing_summary = df.isnull().sum()
missing_cols = missing_summary[missing_summary > 0]
if len(missing_cols) > 0:
    for col, count in missing_cols.items():
        print(f"  {col}: {count} ({count/len(df)*100:.2f}%)")
else:
    print("  No missing values!")

# Fill missing values for lag features with forward fill (appropriate for time series)
lag_cols = [col for col in df.columns if 'Lag' in col or 'MA' in col]
for col in lag_cols:
    df[col] = df[col].ffill()
    df[col] = df[col].bfill()

# Fill remaining missing values
df.fillna(0, inplace=True)

print(f"\nâœ“ Missing values handled using forward/backward fill for time series")

# Remove temporary columns
if 'Year_Month' in df.columns:
    df = df.drop('Year_Month', axis=1)

# Save engineered dataset
df.to_csv('Crop_Price_Features_Engineered.csv', index=False)
print(f"\nâœ“ Feature-engineered dataset saved: Crop_Price_Features_Engineered.csv")

# ============================================================================
# STEP 10: FEATURE SUMMARY
# ============================================================================
print("\n" + "=" * 80)
print("FEATURE ENGINEERING SUMMARY")
print("=" * 80)

print(f"""
ðŸ“Š FEATURE SUMMARY:

Original Features: 20
New Features Created: {df.shape[1] - 20}
Total Features: {df.shape[1]}
Total Records: {len(df):,}

FEATURE CATEGORIES:

1. Rolling Averages (3 features):
   â€¢ Price_MA_7, Price_MA_30, Price_MA_Ratio
   â€¢ Smooth noise and capture trends

2. Price Change Features (4 features):
   â€¢ Price_Change_Pct, Price_Change_7d_Pct, Price_Change_30d_Pct, Price_Volatility
   â€¢ Measure momentum and volatility

3. Temporal Features (13+ features):
   â€¢ Day/Week/Month/Quarter/Season indicators
   â€¢ Cyclical encodings
   â€¢ Capture seasonality and cycles

4. Rainfall Deviation Features (8+ features):
   â€¢ Rainfall_Deviation, Rainfall_Deviation_Pct, Cumulative_Rainfall_Month
   â€¢ Rainfall categories
   â€¢ Context-aware weather impact

5. Lag Features (8+ features):
   â€¢ Price lags: 1, 3, 7, 14, 30 days
   â€¢ Arrivals lags: 1, 7, 14 days
   â€¢ Provide historical context

6. Advanced Features (10+ features):
   â€¢ Price spread, temperature range, comfort index
   â€¢ Log transformations
   â€¢ Interaction features

7. Encoded Variables (3 features):
   â€¢ District, Commodity, Market encoding
   â€¢ Machine-readable categorical variables
""")

# Display sample of engineered features
print("\n" + "=" * 80)
print("SAMPLE OF ENGINEERED FEATURES")
print("=" * 80)

feature_cols = ['Modal_Price', 'Price_MA_7', 'Price_MA_30', 'Price_Change_Pct', 
                'Rainfall_Deviation', 'Price_Lag_1d', 'Price_Lag_7d', 'Season']
print(df[feature_cols].head(10).to_string())

print("\n" + "=" * 80)
print("âœ… FEATURE ENGINEERING COMPLETED SUCCESSFULLY!")
print("=" * 80)

print("""
ðŸŽ¯ NEXT STEPS FOR ML MODEL TRAINING:

1. FEATURE SELECTION:
   â€¢ Use correlation analysis to remove redundant features
   â€¢ Apply feature importance from tree-based models
   â€¢ Consider domain knowledge for feature subset

2. FEATURE SCALING:
   â€¢ Normalize/standardize continuous features
   â€¢ Keep encoded categorical features as-is
   â€¢ Consider separate scaling for different feature groups

3. TRAIN-TEST SPLIT:
   â€¢ Use time-based split (e.g., first 80% train, last 20% test)
   â€¢ Maintain temporal order - no random shuffling
   â€¢ Consider multiple validation periods

4. MODEL SELECTION:
   â€¢ Start with tree-based models (XGBoost, Random Forest)
   â€¢ Try time series models (ARIMA, Prophet)
   â€¢ Experiment with deep learning (LSTM, Transformer)

5. EVALUATION METRICS:
   â€¢ MAE, RMSE for price prediction accuracy
   â€¢ MAPE for percentage error
   â€¢ Directional accuracy (up/down prediction)
""")

print("=" * 80)
